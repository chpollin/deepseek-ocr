#!/usr/bin/env python3
"""
Create sample subset for GitHub Pages deployment
- Select 5-10 representative pages with images
- Generate full report with all statistics
- Provide complete transcription as downloadable file
"""

import io, sys
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

import json
import os
from pathlib import Path
import shutil

def load_ocr_results(results_file):
    with open(results_file, 'r', encoding='utf-8') as f:
        return json.load(f)

def select_sample_pages(pages, max_samples=10):
    """Select representative sample pages evenly distributed"""
    total = len(pages)

    if total <= max_samples:
        return pages, list(range(total))

    # Select evenly distributed samples
    step = total / max_samples
    sample_indices = [int(i * step) for i in range(max_samples)]

    # Always include first and last page
    if 0 not in sample_indices:
        sample_indices[0] = 0
    if total - 1 not in sample_indices:
        sample_indices[-1] = total - 1

    sample_pages = [pages[i] for i in sample_indices]

    return sample_pages, sample_indices

def generate_report(doc_id, full_data, sample_indices):
    """Generate comprehensive report"""

    meta = full_data.get('metadata') or full_data.get('mets_metadata', {})
    pages = full_data.get('pages', [])

    total_chars = sum(p.get('characters', 0) for p in pages)
    total_filtered = sum(p.get('filtered', 0) for p in pages)
    total_time = sum(p.get('time_seconds', 0) for p in pages)

    avg_chars_per_page = total_chars / len(pages) if pages else 0
    avg_time_per_page = total_time / len(pages) if pages else 0

    report = f"""# OCR Analysis Report: {meta.get('title', doc_id)}

## Document Information

- **Title:** {meta.get('title', 'N/A')}
- **Author:** {meta.get('author', 'N/A')}
- **Language:** {meta.get('language', 'N/A')}
- **Signature:** {meta.get('signature', 'N/A')}
- **Total Pages:** {len(pages)}

## Processing Statistics

- **Total Characters Recognized:** {total_chars:,}
- **Characters Filtered (Artifacts):** {total_filtered:,} ({total_filtered/total_chars*100:.1f}%)
- **Total Processing Time:** {total_time:.1f}s ({total_time/60:.1f} minutes)
- **Average per Page:** {avg_chars_per_page:.0f} chars, {avg_time_per_page:.1f}s

## Sample Pages in Viewer

The interactive viewer displays {len(sample_indices)} representative pages:

"""

    for idx, page_num in enumerate(sample_indices, 1):
        page = pages[page_num]
        report += f"- **Sample {idx}:** Page {page_num + 1} ({page.get('characters', 0)} chars, {page.get('time_seconds', 0):.1f}s)\n"

    report += f"""

## Page-by-Page Statistics

| Page | Characters | Filtered | Time (s) | Rate (chars/s) |
|------|------------|----------|----------|----------------|
"""

    for i, page in enumerate(pages, 1):
        chars = page.get('characters', 0)
        filtered = page.get('filtered', 0)
        time_s = page.get('time_seconds', 0)
        rate = chars / time_s if time_s > 0 else 0
        report += f"| {i} | {chars} | {filtered} | {time_s:.1f} | {rate:.0f} |\n"

    report += f"""

## Complete Transcription

The full transcription of all {len(pages)} pages is available as:
- `{doc_id}_transcription.txt` - Plain text format
- `{doc_id}_full.json` - Complete JSON with metadata

## Technical Details

- **OCR Model:** DeepSeek-OCR (3B parameters)
- **Precision:** BF16
- **Hardware:** NVIDIA GPU
- **Artifact Filtering:** Pattern-based detection

---

*Generated by DeepSeek-OCR Analysis Pipeline*
"""

    return report

def create_transcription_file(doc_id, pages, output_dir):
    """Create complete transcription as TXT file"""

    transcription = []

    for i, page in enumerate(pages, 1):
        transcription.append(f"{'='*60}")
        transcription.append(f"PAGE {i}")
        transcription.append(f"{'='*60}")
        transcription.append("")

        text = page.get('text', '') or page.get('filtered_text', '')
        transcription.append(text)
        transcription.append("")
        transcription.append("")

    txt_file = output_dir / f"{doc_id}_transcription.txt"
    with open(txt_file, 'w', encoding='utf-8') as f:
        f.write('\n'.join(transcription))

    return txt_file

def copy_sample_images(doc_id, full_data, sample_indices, output_dir):
    """Copy sample images to deployment folder"""

    images_dir = output_dir / 'images' / doc_id
    images_dir.mkdir(parents=True, exist_ok=True)

    pages = full_data.get('pages', [])

    for sample_idx in sample_indices:
        page = pages[sample_idx]

        # Determine source image path
        if 'image_file' in page:
            # METS format
            urn = full_data.get('mets_metadata', {}).get('urn', '')
            if urn:
                object_id = urn.split('/')[-1].replace(':', '_')
                src_path = Path('data') / object_id / 'images' / page['image_file']
        elif 'file' in page:
            # PDF format
            src_path = Path(page['file'])
        else:
            continue

        if src_path.exists():
            dest_path = images_dir / src_path.name
            shutil.copy2(src_path, dest_path)
            print(f"  ‚úì Copied: {src_path.name}")

def create_sample_dataset(doc_id, full_data, output_dir, max_samples=10):
    """Create complete sample dataset"""

    print(f"\n{'='*60}")
    print(f"Creating sample dataset: {doc_id}")
    print(f"{'='*60}")

    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    pages = full_data.get('pages', [])
    print(f"Total pages: {len(pages)}")

    # Select sample pages
    sample_pages, sample_indices = select_sample_pages(pages, max_samples)
    print(f"Sample pages: {len(sample_pages)} (indices: {sample_indices})")

    # Create sampled JSON for viewer
    sampled_data = full_data.copy()
    sampled_data['pages'] = sample_pages
    sampled_data['is_sample'] = True
    sampled_data['total_pages'] = len(pages)
    sampled_data['sample_indices'] = sample_indices

    sample_json = output_dir / f"{doc_id}_sample.json"
    with open(sample_json, 'w', encoding='utf-8') as f:
        json.dump(sampled_data, f, ensure_ascii=False, indent=2)
    print(f"‚úì Created: {sample_json.name}")

    # Create full JSON
    full_json = output_dir / f"{doc_id}_full.json"
    with open(full_json, 'w', encoding='utf-8') as f:
        json.dump(full_data, f, ensure_ascii=False, indent=2)
    print(f"‚úì Created: {full_json.name}")

    # Create transcription
    txt_file = create_transcription_file(doc_id, pages, output_dir)
    print(f"‚úì Created: {txt_file.name}")

    # Copy sample images
    print(f"Copying {len(sample_indices)} sample images...")
    copy_sample_images(doc_id, full_data, sample_indices, output_dir)

    # Generate report
    report = generate_report(doc_id, full_data, sample_indices)
    report_file = output_dir / f"{doc_id}_report.md"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)
    print(f"‚úì Created: {report_file.name}")

    print(f"\n‚úÖ Sample dataset complete for {doc_id}")
    print(f"   - {len(sample_pages)} pages in viewer")
    print(f"   - {len(pages)} pages in full transcription")
    print(f"   - All files in: {output_dir}")

    return sampled_data

def main():
    """Process all OCR results and create samples"""

    results_dir = Path('results')
    output_dir = Path('samples')

    if not results_dir.exists():
        print("‚ùå No results/ directory found")
        return

    # Find all cleaned OCR results
    all_documents = {}

    for result_file in results_dir.glob('*/*_ocr_cleaned.json'):
        doc_id = result_file.stem.replace('_ocr_cleaned', '')
        try:
            data = load_ocr_results(result_file)
            all_documents[doc_id] = data
            print(f"‚úì Found: {doc_id} ({len(data.get('pages', []))} pages)")
        except Exception as e:
            print(f"‚ö† Error loading {result_file}: {e}")

    if not all_documents:
        print("‚ùå No OCR results found")
        return

    # Create samples for each document
    sampled_documents = {}

    for doc_id, full_data in all_documents.items():
        sampled_data = create_sample_dataset(doc_id, full_data, output_dir, max_samples=10)
        sampled_documents[doc_id] = sampled_data

    # Create master index
    print(f"\n{'='*60}")
    print(f"Creating master sample index...")
    print(f"{'='*60}")

    master_json = output_dir / 'samples.json'
    with open(master_json, 'w', encoding='utf-8') as f:
        json.dump(sampled_documents, f, ensure_ascii=False, indent=2)
    print(f"‚úì Created: {master_json}")

    print(f"\nüéâ All samples created!")
    print(f"üìÇ Output: {output_dir.absolute()}")
    print(f"\nüìä Summary:")
    for doc_id, data in sampled_documents.items():
        print(f"   - {doc_id}: {len(data['pages'])} sample pages (of {data['total_pages']} total)")

if __name__ == '__main__':
    main()
